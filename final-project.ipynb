{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":558442,"sourceType":"datasetVersion","datasetId":268405},{"sourceId":7270544,"sourceType":"datasetVersion","datasetId":4214636},{"sourceId":7291663,"sourceType":"datasetVersion","datasetId":4229035},{"sourceId":7291898,"sourceType":"datasetVersion","datasetId":4229180},{"sourceId":7292221,"sourceType":"datasetVersion","datasetId":4229397},{"sourceId":7314855,"sourceType":"datasetVersion","datasetId":4244795},{"sourceId":7323992,"sourceType":"datasetVersion","datasetId":4250693},{"sourceId":7326220,"sourceType":"datasetVersion","datasetId":4252279}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf### models\nimport pandas as pd ### reading and processing data\nimport numpy as np### math computations\nimport matplotlib.pyplot as plt### plotting bar chart\nimport tensorflow_datasets as tfds\nimport os\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom keras.preprocessing import image\n#import seaborn as sns ### visualization\n# if you want to use TPU comment seaborn lib","metadata":{"id":"Hf4r4MfDgCCr","_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-01-02T21:13:29.059472Z","iopub.execute_input":"2024-01-02T21:13:29.060004Z","iopub.status.idle":"2024-01-02T21:13:29.064878Z","shell.execute_reply.started":"2024-01-02T21:13:29.059967Z","shell.execute_reply":"2024-01-02T21:13:29.063961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-12-24T12:51:57.942363Z","iopub.execute_input":"2023-12-24T12:51:57.942833Z","iopub.status.idle":"2023-12-24T12:52:54.739953Z","shell.execute_reply.started":"2023-12-24T12:51:57.942798Z","shell.execute_reply":"2023-12-24T12:52:54.738454Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plant='/kaggle/input/plantv'\ncategories = os.listdir('/kaggle/input/plantv/Plant_leave_diseases_dataset_with_augmentation')\nprint(\"List of categories = \",categories,\"\\n\\nNo. of categories = \", len(categories))\n#print classes in plant village dataset ","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:13:37.625975Z","iopub.execute_input":"2024-01-02T21:13:37.626332Z","iopub.status.idle":"2024-01-02T21:13:37.660678Z","shell.execute_reply.started":"2024-01-02T21:13:37.626302Z","shell.execute_reply":"2024-01-02T21:13:37.660033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating palnt village dataframe (path,class)\nplant_dataset_path = '/kaggle/input/plantv/Plant_leave_diseases_dataset_with_augmentation'\n\n# Create lists to store file paths and corresponding classes\nfile_paths = []\nclasses = []\n\n# Iterate through each class directory\nfor class_folder in os.listdir(plant_dataset_path):\n    class_path = os.path.join(plant_dataset_path, class_folder)\n    \n    # Check if it's a directory\n    if os.path.isdir(class_path):\n        # Iterate through each image file in the class directory\n        for image_file in os.listdir(class_path):\n            # Construct the full file path\n            image_path = os.path.join(class_path, image_file)\n            \n            # Append the file path and class to the lists\n            file_paths.append(image_path)\n            classes.append(class_folder)\n\n# Create a DataFrame\ndf = pd.DataFrame({'file_path': file_paths, 'class': classes})\n\n# Display the DataFrame\nlen(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:14:15.003196Z","iopub.execute_input":"2024-01-02T21:14:15.004079Z","iopub.status.idle":"2024-01-02T21:14:15.141392Z","shell.execute_reply.started":"2024-01-02T21:14:15.004040Z","shell.execute_reply":"2024-01-02T21:14:15.140727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"class\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_row = df.sample()\n\n# Get the file path and class label\nfile_path = random_row['file_path'].values[0]\nclass_label = random_row['class'].values[0]\n\n# Open the image using PIL\nimage = Image.open(file_path)\n\n# Display the image\nplt.imshow(image)\nplt.title(f\"Class: {class_label}\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-27T16:01:14.993215Z","iopub.execute_input":"2023-12-27T16:01:14.993673Z","iopub.status.idle":"2023-12-27T16:01:15.307289Z","shell.execute_reply.started":"2023-12-27T16:01:14.993636Z","shell.execute_reply":"2023-12-27T16:01:15.306378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#rice **dataset****","metadata":{}},{"cell_type":"code","source":"train_dataset_path='/kaggle/input/riceleafs/RiceLeafs/train'\nval_dataset_path='/kaggle/input/riceleafs/RiceLeafs/validation'","metadata":{"id":"5Jyk11alH987","execution":{"iopub.status.busy":"2024-01-02T21:15:29.349005Z","iopub.execute_input":"2024-01-02T21:15:29.349384Z","iopub.status.idle":"2024-01-02T21:15:29.353094Z","shell.execute_reply.started":"2024-01-02T21:15:29.349342Z","shell.execute_reply":"2024-01-02T21:15:29.352395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creat rice dataframe \n# Function to get a DataFrame from a directory\ndef create_dataframe_from_directory(directory):\n    file_paths = []\n    class_labels = []\n    for class_label in os.listdir(directory):\n        class_path = os.path.join(directory, class_label)\n        if os.path.isdir(class_path):\n            for filename in os.listdir(class_path):\n                file_path = os.path.join(class_path, filename)\n                file_paths.append(file_path)\n                class_labels.append(class_label)\n    return pd.DataFrame({'file_path': file_paths, 'class': class_labels})\n\n# Create DataFrames for the training and validation datasets\ndf_train = create_dataframe_from_directory(train_dataset_path)\ndf_val = create_dataframe_from_directory(val_dataset_path)\n\n# Concatenate the DataFrames to create a unified dataset\nrice_df = pd.concat([df_train, df_val], ignore_index=True)\n\n# Display the first few rows of the combined DataFrame\nprint(rice_df.head())\n\n# Optional: Save the combined DataFrame to a CSV file\nrice_df.to_csv('/kaggle/working/combined_dataset.csv', index=False)\nlen(rice_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:15:29.567848Z","iopub.execute_input":"2024-01-02T21:15:29.568143Z","iopub.status.idle":"2024-01-02T21:15:30.702797Z","shell.execute_reply.started":"2024-01-02T21:15:29.568116Z","shell.execute_reply":"2024-01-02T21:15:30.702138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rice_df[\"class\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-27T16:02:03.906475Z","iopub.execute_input":"2023-12-27T16:02:03.907568Z","iopub.status.idle":"2023-12-27T16:02:03.916703Z","shell.execute_reply.started":"2023-12-27T16:02:03.907523Z","shell.execute_reply":"2023-12-27T16:02:03.915495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_row = rice_df.sample()\n\n# Get the file path and class label\nfile_path = random_row['file_path'].values[0]\nclass_label = random_row['class'].values[0]\n\n# Open the image using PIL\nimage = Image.open(file_path)\n\n# Display the image\nplt.imshow(image)\nplt.title(f\"Class: {class_label}\")\nplt.axis('off')\nplt.show()","metadata":{"id":"SlMbV_9HDJ-Y","outputId":"a682df42-2959-45fa-e956-de034aac7c04","execution":{"iopub.status.busy":"2023-12-27T16:02:10.733861Z","iopub.execute_input":"2023-12-27T16:02:10.734278Z","iopub.status.idle":"2023-12-27T16:02:11.325539Z","shell.execute_reply.started":"2023-12-27T16:02:10.734248Z","shell.execute_reply":"2023-12-27T16:02:11.321180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concat village dataframe with rice dataframe \ndf_final = pd.concat([df, rice_df], ignore_index=True)\ndf_final.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-27T16:02:19.683027Z","iopub.execute_input":"2023-12-27T16:02:19.683481Z","iopub.status.idle":"2023-12-27T16:02:19.697248Z","shell.execute_reply.started":"2023-12-27T16:02:19.683443Z","shell.execute_reply":"2023-12-27T16:02:19.695984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_final)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T16:46:09.639973Z","iopub.execute_input":"2023-12-27T16:46:09.640448Z","iopub.status.idle":"2023-12-27T16:46:09.647361Z","shell.execute_reply.started":"2023-12-27T16:46:09.640406Z","shell.execute_reply":"2023-12-27T16:46:09.646491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resize dataframe and conver all images to RGB\n# Set the output directory for resized images in the working directory\n#save the output \noutput_directory = '/kaggle/working/deep_learning/resizeV'\n\n# Ensure the output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Batch size for processing\nbatch_size = 50  # You can adjust this based on your memory constraints\n\n# Calculate the number of batches\nnum_batches = len(df_final) // batch_size + 1\n\n# Helper function to handle RGBA images\ndef handle_rgba(image):\n    if image.mode == 'RGBA':\n        # Remove alpha channel before resizing\n        return image.convert('RGB')\n    return image\n\n# Iterate over batches\nfor i in range(num_batches):\n    start_idx = i * batch_size\n    end_idx = (i + 1) * batch_size\n\n    # Slice the DataFrame for the current batch\n    batch_df = df_final.iloc[start_idx:end_idx]\n\n    # Iterate over each row in the batch\n    for index, row in batch_df.iterrows():\n        input_path = row['file_path']\n        class_label = row['class']\n\n        # Open and handle RGBA images\n        with Image.open(input_path) as img:\n            img = handle_rgba(img)\n\n            # Resize the image\n            resized_img = img.resize((300, 300))\n\n            # Save the resized image to the output directory with the original format\n            output_filename = f\"{class_label}_{index}{os.path.splitext(input_path)[-1]}\"  # Keep the original file extension\n            output_path = os.path.join(output_directory, output_filename)\n            resized_img.save(output_path)\n\n# Create a ZIP archive of the resized images\nshutil.make_archive('/kaggle/working/deep_learning_resized_images', 'zip', output_directory)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T16:03:19.853495Z","iopub.execute_input":"2023-12-27T16:03:19.853974Z","iopub.status.idle":"2023-12-27T16:19:19.373973Z","shell.execute_reply.started":"2023-12-27T16:03:19.853939Z","shell.execute_reply":"2023-12-27T16:19:19.372512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"******************************\n","metadata":{}},{"cell_type":"markdown","source":"##############################\n","metadata":{}},{"cell_type":"markdown","source":"******************************","metadata":{}},{"cell_type":"code","source":"#creat a dataframe \npaths = []\nclasses = []\n\n# Iterate over files in the resize folder\nfor filename in os.listdir('/kaggle/input/resizev'):\n    # Extract class and index from the filename\n    parts = os.path.splitext(filename)[0].split('_')\n    class_label = parts[0]\n    index = parts[1]\n\n    # Create the full path\n    full_path = os.path.join('/kaggle/input/resizev', filename)\n\n    # Append to the lists\n    classes.append(class_label)\n    paths.append(full_path)\n\n# Create a DataFrame\ndf_resized = pd.DataFrame({'path': paths, 'class': classes})\n\ndf_resized.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:40:45.516033Z","iopub.execute_input":"2024-01-02T21:40:45.516397Z","iopub.status.idle":"2024-01-02T21:40:46.627129Z","shell.execute_reply.started":"2024-01-02T21:40:45.516358Z","shell.execute_reply":"2024-01-02T21:40:46.626413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fixing the dataframe classes\n# Function to extract class label from image path\ndef extract_class_from_path(image_path):\n\n    # Extract the filename without the extension\n    filename = os.path.splitext(os.path.basename(image_path))[0]\n    \n    # Extract the class label from the filename\n    # Assuming the class label is everything before the last underscore\n    class_label = filename.rsplit('_', 1)[0]\n    \n    # If the class label contains \"image\", extract the number in parentheses\n    if \"image\" in class_label:\n        class_label = \"image_\" + ''.join(filter(str.isdigit, class_label))\n\n    return class_label\n\n# Apply the function to create a new 'class' column\ndf_resized['class'] = df_resized['path'].apply(extract_class_from_path)\n\n# Display the updated DataFrame\ndf_resized['class'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:40:47.913053Z","iopub.execute_input":"2024-01-02T21:40:47.913872Z","iopub.status.idle":"2024-01-02T21:40:48.071381Z","shell.execute_reply.started":"2024-01-02T21:40:47.913824Z","shell.execute_reply":"2024-01-02T21:40:48.070632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_classes = df_resized['class'].unique()\n\n# Display the unique values\nprint(len(unique_classes))","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:40:48.359682Z","iopub.execute_input":"2024-01-02T21:40:48.359992Z","iopub.status.idle":"2024-01-02T21:40:48.368598Z","shell.execute_reply.started":"2024-01-02T21:40:48.359965Z","shell.execute_reply":"2024-01-02T21:40:48.367839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_resized.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:40:48.974042Z","iopub.execute_input":"2024-01-02T21:40:48.974772Z","iopub.status.idle":"2024-01-02T21:40:48.982213Z","shell.execute_reply.started":"2024-01-02T21:40:48.974734Z","shell.execute_reply":"2024-01-02T21:40:48.981510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef process_image(image_path):\n    # Load image using PIL\n    img = Image.open(image_path)\n\n    # Convert image to array\n    img_array = np.array(img)\n\n    # Normalize the image using EfficientNet B3 preprocessing\n    img_array = preprocess_input(np.expand_dims(img_array, axis=0))\n\n    return img_array\n\n# Apply the process_image function to each row in the DataFrame\ndf_resized['processed_image'] = df_resized['path'].apply(process_image)\n","metadata":{"id":"Ilmkcnut6wlA","execution":{"iopub.status.busy":"2024-01-02T21:40:55.549343Z","iopub.execute_input":"2024-01-02T21:40:55.549699Z","iopub.status.idle":"2024-01-02T21:48:41.340905Z","shell.execute_reply.started":"2024-01-02T21:40:55.549669Z","shell.execute_reply":"2024-01-02T21:48:41.340029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_resized.iloc[:, 1:3]\nlen(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:48:48.355041Z","iopub.execute_input":"2024-01-02T21:48:48.355428Z","iopub.status.idle":"2024-01-02T21:48:48.364560Z","shell.execute_reply.started":"2024-01-02T21:48:48.355396Z","shell.execute_reply":"2024-01-02T21:48:48.363805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tensors \ndata[\"processed_image\"][1].shape","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:48:48.642428Z","iopub.execute_input":"2024-01-02T21:48:48.642741Z","iopub.status.idle":"2024-01-02T21:48:48.647881Z","shell.execute_reply.started":"2024-01-02T21:48:48.642714Z","shell.execute_reply":"2024-01-02T21:48:48.647131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = data[\"class\"].value_counts()\ncount.plot.bar()\nplt.ylabel('Number of records')\nplt.xlabel('Target Class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:48:49.334943Z","iopub.execute_input":"2024-01-02T21:48:49.335301Z","iopub.status.idle":"2024-01-02T21:48:49.750082Z","shell.execute_reply.started":"2024-01-02T21:48:49.335268Z","shell.execute_reply":"2024-01-02T21:48:49.749295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import resample\nimport pandas as pd\n\n# Assuming df is your DataFrame\n# Replace the class labels with your actual class labels\nundersample_classes = [\n    'Orange___Haunglongbing_(Citrus_greening)',\n    'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n    'Soybean___healthy',\n    'Peach___Bacterial_spot',\n    'Tomato___Bacterial_spot',\n    'Tomato___Late_blight',\n    'Squash___Powdery_mildew',\n    'Tomato___Septoria_leaf_spot',\n    'Tomato___Spider_mites Two-spotted_spider_mite',\n    'Apple___healthy',\n    'Tomato___healthy',\n    'Blueberry___healthy',\n    'Healthy',\n    'Pepper,_bell___healthy',\n    'Tomato___Target_Spot',\n    'Grape___Esca_(Black_Measles)',\n    'Corn___Common_rust',\n    'Grape___Black_rot',\n    'Corn___healthy',\n    'Background_without_leaves'\n]\n\noversample_classes = ['LeafBlast', 'Hispa', 'BrownSpot']\ndesired_undersample_samples = 1000\ndesired_oversample_samples = 1000\n\n# Separate classes for undersampling\ndf_undersample = data[data['class'].isin(undersample_classes)]\n\n# Undersample specified classes\ndf_undersampled = pd.concat([resample(df_undersample[df_undersample['class'] == cls],\n                                       replace=False,\n                                       n_samples=desired_undersample_samples,\n                                       random_state=42) for cls in undersample_classes])\n\n# Separate classes for oversampling\ndf_oversample = data[data['class'].isin(oversample_classes)]\n\n# Oversample specified classes\ndf_oversampled = pd.concat([resample(df_oversample[df_oversample['class'] == cls],\n                                      replace=True,\n                                      n_samples=desired_oversample_samples,\n                                      random_state=42) for cls in oversample_classes])\n\n# Leave other classes unchanged\ndf_remaining = data[~data['class'].isin(undersample_classes + oversample_classes)]\n\n# Combine undersampled, oversampled, and remaining classes\ndata = pd.concat([df_undersampled, df_oversampled, df_remaining])\n\n# Shuffle the DataFrame\ndata = data.sample(frac=1, random_state=42)\n\n# Print the value counts to verify the sampling\nprint(data['class'].value_counts())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:48:53.081317Z","iopub.execute_input":"2024-01-02T21:48:53.082117Z","iopub.status.idle":"2024-01-02T21:48:53.204301Z","shell.execute_reply.started":"2024-01-02T21:48:53.082082Z","shell.execute_reply":"2024-01-02T21:48:53.203483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = data[\"class\"].value_counts()\ncount.plot.bar()\nplt.ylabel('Number of records')\nplt.xlabel('Target Class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:08:14.937504Z","iopub.execute_input":"2024-01-02T16:08:14.937904Z","iopub.status.idle":"2024-01-02T16:08:15.516388Z","shell.execute_reply.started":"2024-01-02T16:08:14.937871Z","shell.execute_reply":"2024-01-02T16:08:15.515239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[\"processed_image\"]),len(data[\"class\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:08:15.518164Z","iopub.execute_input":"2024-01-02T16:08:15.519095Z","iopub.status.idle":"2024-01-02T16:08:15.526292Z","shell.execute_reply.started":"2024-01-02T16:08:15.519061Z","shell.execute_reply":"2024-01-02T16:08:15.524971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dont run this \n#creating zip file to donwload data \n#shutil.make_archive('/kaggle/1/final_datav', 'zip', )\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:08:15.862136Z","iopub.execute_input":"2024-01-02T16:08:15.863004Z","iopub.status.idle":"2024-01-02T16:08:15.868685Z","shell.execute_reply.started":"2024-01-02T16:08:15.862956Z","shell.execute_reply":"2024-01-02T16:08:15.867372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data=pd.read_csv(\"/kaggle/input/final-datav/final_datav.csv\")\n#data.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:08:16.189570Z","iopub.execute_input":"2024-01-02T16:08:16.189989Z","iopub.status.idle":"2024-01-02T16:08:16.195271Z","shell.execute_reply.started":"2024-01-02T16:08:16.189955Z","shell.execute_reply":"2024-01-02T16:08:16.193989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB3\nfrom sklearn.preprocessing import LabelEncoder\n\n#imagenet weights \nefficientnetb3_weights_path = '/kaggle/input/efnetb3/efficientnetb3_notop.h5'\n\n# Build the EfficientNetB3 model\nbase_model = EfficientNetB3(weights=efficientnetb3_weights_path, include_top=False, input_shape=(300, 300, 3))\nbase_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:07.404115Z","iopub.execute_input":"2024-01-02T21:49:07.404859Z","iopub.status.idle":"2024-01-02T21:49:11.040139Z","shell.execute_reply.started":"2024-01-02T21:49:07.404814Z","shell.execute_reply":"2024-01-02T21:49:11.039271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndata['class_encoded'] = label_encoder.fit_transform(data['class'])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:25.627353Z","iopub.execute_input":"2024-01-02T21:49:25.627730Z","iopub.status.idle":"2024-01-02T21:49:25.646446Z","shell.execute_reply.started":"2024-01-02T21:49:25.627701Z","shell.execute_reply":"2024-01-02T21:49:25.645703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training (80%), validation (10%), and test (10%)\ntrain_df, temp_df = train_test_split(data, test_size=0.2, random_state=42)\nvalid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:26.158217Z","iopub.execute_input":"2024-01-02T21:49:26.158886Z","iopub.status.idle":"2024-01-02T21:49:26.173638Z","shell.execute_reply.started":"2024-01-02T21:49:26.158847Z","shell.execute_reply":"2024-01-02T21:49:26.172771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reset index for each subset\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:32.139195Z","iopub.execute_input":"2024-01-02T21:49:32.139918Z","iopub.status.idle":"2024-01-02T21:49:32.151077Z","shell.execute_reply.started":"2024-01-02T21:49:32.139871Z","shell.execute_reply":"2024-01-02T21:49:32.150136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the number of classes based on your dataset\nnum_classes = len(data['class'].unique())\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:36.905416Z","iopub.execute_input":"2024-01-02T21:49:36.905739Z","iopub.status.idle":"2024-01-02T21:49:36.916644Z","shell.execute_reply.started":"2024-01-02T21:49:36.905712Z","shell.execute_reply":"2024-01-02T21:49:36.915955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract NumPy arrays from the processed_image column\ntrain_images = np.array(train_df['processed_image'].to_list())\nvalid_images = np.array(valid_df['processed_image'].to_list())\ntest=np.array(test_df['processed_image'].to_list())","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:56:34.427292Z","iopub.execute_input":"2024-01-02T21:56:34.427652Z","iopub.status.idle":"2024-01-02T21:56:37.897088Z","shell.execute_reply.started":"2024-01-02T21:56:34.427623Z","shell.execute_reply":"2024-01-02T21:56:37.895836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract class labels\ntrain_labels = train_df['class_encoded'].to_numpy()\nvalid_labels = valid_df['class_encoded'].to_numpy()\ntest_labels=test_df['class_encoded'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:57:03.441628Z","iopub.execute_input":"2024-01-02T21:57:03.442550Z","iopub.status.idle":"2024-01-02T21:57:03.446757Z","shell.execute_reply.started":"2024-01-02T21:57:03.442512Z","shell.execute_reply":"2024-01-02T21:57:03.445922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the input images\ntrain_images = train_images.reshape((-1,300,300,3))\nvalid_images = valid_images.reshape((-1,300,300,3))\ntest=test.reshape((-1,300,300,3))","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:57:25.116883Z","iopub.execute_input":"2024-01-02T21:57:25.117214Z","iopub.status.idle":"2024-01-02T21:57:25.122085Z","shell.execute_reply.started":"2024-01-02T21:57:25.117187Z","shell.execute_reply":"2024-01-02T21:57:25.121208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# improve performance by allowing certain computations to be compiled at runtime.\ntf.config.optimizer.set_jit(True)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:57:30.981918Z","iopub.execute_input":"2024-01-02T21:57:30.982238Z","iopub.status.idle":"2024-01-02T21:57:30.986484Z","shell.execute_reply.started":"2024-01-02T21:57:30.982211Z","shell.execute_reply":"2024-01-02T21:57:30.985629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nmodel = models.Sequential([\n    layers.InputLayer(input_shape=(300, 300, 3)),\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(43, activation='softmax')\n])","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:53.799318Z","iopub.execute_input":"2024-01-02T21:49:53.799657Z","iopub.status.idle":"2024-01-02T21:49:55.061734Z","shell.execute_reply.started":"2024-01-02T21:49:53.799628Z","shell.execute_reply":"2024-01-02T21:49:55.060678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.01),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:49:55.339852Z","iopub.execute_input":"2024-01-02T21:49:55.340215Z","iopub.status.idle":"2024-01-02T21:49:55.357741Z","shell.execute_reply.started":"2024-01-02T21:49:55.340183Z","shell.execute_reply":"2024-01-02T21:49:55.356775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:09:04.441778Z","iopub.execute_input":"2024-01-02T16:09:04.442367Z","iopub.status.idle":"2024-01-02T16:09:04.502724Z","shell.execute_reply.started":"2024-01-02T16:09:04.442333Z","shell.execute_reply":"2024-01-02T16:09:04.501930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint_filepath = '/kaggle/working/best_model_epoch_{epoch:02d}.h5'\nmodel_checkpoint = ModelCheckpoint(\n    checkpoint_filepath,\n    save_best_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:09:10.555613Z","iopub.execute_input":"2024-01-02T16:09:10.556591Z","iopub.status.idle":"2024-01-02T16:09:10.563240Z","shell.execute_reply.started":"2024-01-02T16:09:10.556550Z","shell.execute_reply":"2024-01-02T16:09:10.561970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the batch size and image size\n# Set the number of epochs and bathces \n#TPU can handle many bathces \nbatch_size=1000\nimage_size=(300,300)\nepochs=5","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:00:16.542397Z","iopub.execute_input":"2024-01-02T21:00:16.542905Z","iopub.status.idle":"2024-01-02T21:00:16.547222Z","shell.execute_reply.started":"2024-01-02T21:00:16.542859Z","shell.execute_reply":"2024-01-02T21:00:16.546311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n# EAT \nclass TimeEstimationCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start_time = time.time()\n\n    def on_epoch_end(self, epoch, logs=None):\n        elapsed_time = time.time() - self.epoch_start_time\n        remaining_time = elapsed_time * (epochs - epoch - 1)\n        print(f'Epoch {epoch + 1}/{epochs} - ETA: {remaining_time:.2f} seconds')","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:09:15.613013Z","iopub.execute_input":"2024-01-02T16:09:15.613811Z","iopub.status.idle":"2024-01-02T16:09:15.621399Z","shell.execute_reply.started":"2024-01-02T16:09:15.613765Z","shell.execute_reply":"2024-01-02T16:09:15.620623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save epoch information\nclass DetailedHistory(Callback):\n    def __init__(self, filename_prefix):\n        super(DetailedHistory, self).__init__()\n        self.filename_prefix = filename_prefix\n        self.details = {'epochs': [], 'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.details['epochs'].append(epoch + 1)\n        self.details['loss'].append(logs['loss'])\n        self.details['val_loss'].append(logs['val_loss'])\n        self.details['accuracy'].append(logs['accuracy'])\n        self.details['val_accuracy'].append(logs['val_accuracy'])\n\n        # Save details to a CSV file within Kaggle working directory\n        filename = f'/kaggle/working/{self.filename_prefix}_epoch_details.csv'\n        df = pd.DataFrame(self.details)\n        df.to_csv(filename, index=False)\n\n# Create the DetailedHistory callback\ndetailed_history = DetailedHistory('details')\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_estimation_callback = TimeEstimationCallback()\n\nresume_epoch = 1  # Replace with the epoch number you want to resume from\n\n# Load the weights from the saved checkpoint\n#replace every check point here to cont training \nmodel.load_weights(f'/kaggle/working/best_model_epoch_03.h5')\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n# Train the model starting from the next epoch\n  # Replace with the desired number of total epochs\nhistory = model.fit(\n    train_images,\n    train_labels,\n    epochs=5,\n    initial_epoch=1,  # Set the initial epoch to resume training\n    batch_size=batch_size,  # Adjust batch size as needed\n    validation_data=(valid_images, valid_labels),\n    callbacks=[model_checkpoint,time_estimation_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:09:50.443038Z","iopub.execute_input":"2024-01-02T16:09:50.443476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel.save('/kaggle/working/final_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preformance was written  manualy \ndata = {\n    'epochs': [1, 2, 3, 4, 5],\n    'loss': [0.589114, 0.401643, 0.1793, 0.1570, 0.1339],\n    'val_loss': [0.457089, 0.265087, 0.1731, 0.1901, 0.1652],\n    'accuracy': [0.809795, 0.871144, 0.9330, 0.9429, 0.9494],\n    'val_accuracy': [0.862858, 0.913737, 0.9415, 0.9383, 0.9445]\n}\n\ndf = pd.DataFrame(data)\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:29:36.322269Z","iopub.execute_input":"2024-01-02T21:29:36.322680Z","iopub.status.idle":"2024-01-02T21:29:36.331643Z","shell.execute_reply.started":"2024-01-02T21:29:36.322630Z","shell.execute_reply":"2024-01-02T21:29:36.330641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame containing the loss and val_loss data\nepochs = df['epochs']\ntrain_loss = df['loss']\nval_loss = df['val_loss']\n\nplt.plot(epochs, train_loss, label='Training Loss')\nplt.plot(epochs, val_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss over Epochs')\nplt.legend()\nplt.savefig('loss_curve.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:35:14.684900Z","iopub.execute_input":"2024-01-02T21:35:14.685816Z","iopub.status.idle":"2024-01-02T21:35:14.944350Z","shell.execute_reply.started":"2024-01-02T21:35:14.685772Z","shell.execute_reply":"2024-01-02T21:35:14.943411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame containing the accuracy and val_accuracy data\nepochs = df['epochs']\ntrain_accuracy = df['accuracy']\nval_accuracy = df['val_accuracy']\n\nplt.plot(epochs, train_accuracy, label='Training Accuracy')\nplt.plot(epochs, val_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy over Epochs')\nplt.legend()\nplt.savefig('accuracy_curve.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:35:52.397117Z","iopub.execute_input":"2024-01-02T21:35:52.397488Z","iopub.status.idle":"2024-01-02T21:35:52.665638Z","shell.execute_reply.started":"2024-01-02T21:35:52.397459Z","shell.execute_reply":"2024-01-02T21:35:52.664914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the saved model\nsaved_model = load_model('/kaggle/input/finalmodel/final_model.h5')\n\n# Evaluate on the test dataset\ntest_loss, test_accuracy = saved_model.evaluate(test, test_labels)\n\nprint(f'Test Loss: {test_loss}')\nprint(f'Test Accuracy: {test_accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-02T21:57:38.023758Z","iopub.execute_input":"2024-01-02T21:57:38.024123Z","iopub.status.idle":"2024-01-02T22:01:04.347180Z","shell.execute_reply.started":"2024-01-02T21:57:38.024093Z","shell.execute_reply":"2024-01-02T22:01:04.346224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}